---
title: "DATA5711 Bayesian Computational Statistics"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction to R

**Semester 1, 8 February 2021**

**Objectives:**

* Learn to implement linear regression
* Learn to implement gradient descent
* Learn to solve the coefficients in linear regression model analytically
* Learn how to visualise the dataset

We first load the packages required for this tutorial and read in the data contained in the comma-separated values (csv) file.
```{r, warning=FALSE,message=FALSE}
library(plotly)
library(dplyr)
library(reshape2)
# TO DO
X = read_csv("")
Y = 
  
df = cbind(X,Y)
View(df)
p=plot_ly(data=df,x=~feature.1,y=~feature.2,z=~lable.1)%>%add_markers()
##Pairwise Scatterplots
#option 1
pairs(df)
#option 2
par(mfrow=c(1,3))
plot(df$feature.1,df$feature.2,xlab="Feature1",ylab="Feature 2")
plot(df$feature.2,df$label.1,xlab="Feature 2",ylab="Label 1")
plot(df$feature.1,df$label.1,xlab="Feature 1",ylab="Label 1")
```

We can look at the two feature sets studied and the corresponding label:
```{r}
X=as.matrix(X)
```

```{r}
Y=as.matrix(Y)
```

Let's combine the feature sets and labels into a data frame and visualise them in an (interactive) 3D plot:
```{r}
# TO DO
X_with_intercept = cbind(intercept=1,X)
beta= solve(t(X_with_intercept) %*% X_with_intercept) %*% t(X_with_intercept) %*% Y
```

We can also look at the bivariate plots between each pair of variables:
```{r}
# TO DO

```

# Implementing Linear Regression by Calculating the Coefficients Analytically

The forward model of linear regression with two features takes the following form:
$$y_i = \beta_0 + x_{i1}\beta_1 + x_{i2}\beta_2$$
In matrix form it is:
$$y_i = \left[\begin{array}{ccc} 
1 & x_{i1} & x_{i2}
\end{array}\right] \left[\begin{array}{c} 
\beta_0 \\
\beta_1 \\
\beta_2
\end{array}\right] = \boldsymbol{x}^\top_i\boldsymbol{\beta}$$
which can then be extended to include all observations:
$$
\left[\begin{array}{c} 
y_1 \\
\vdots \\
y_n
\end{array}\right]
=
\left[\begin{array}{ccc} 
1 & x_{11} & x_{12} \\ 
\vdots & \vdots & \vdots \\
1 & x_{n1} & x_{n2}
\end{array}\right]
\left[\begin{array}{c} 
\beta_0 \\ 
\beta_1 \\
\beta_2
\end{array}\right]
$$
$$ \boldsymbol{Y} = \boldsymbol{X\beta}$$
The loss function $J(\beta)$ (also called cost function or objective function) for linear regression is:
$$J(\beta)=|| \boldsymbol{Y} - \boldsymbol{X\beta} ||$$
The coefficients in linear regression can be computed analytically by taking the first derivative and solving analytically:
$$\frac{\partial}{\partial \beta}(\boldsymbol{Y} - \boldsymbol{X\beta})^\top(\boldsymbol{Y} - \boldsymbol{X\beta})=0$$
to give
$$\hat{\beta}=(X^\top X)^{-1} X^\top Y,$$
which, for our dataset, is
```{r}
# TO DO

```

We then add the hyperplane to the 3D plot as follows:
```{r}
# TO DO

```

Next, we make predictions on the testing dataset:
```{r}
# TO DO
X_test = read.csv("testing_features.csv")
X_test_with_intercept = cbind(intercept=1, X_test)
X_test_with_intercept = as.matrix(X_test_with_intercept)
prediction = X_test_with_intercept %*% beta
```

and add the predictions to the fitted 3D plot:
```{r}
# TO DO

```

# Implementing Linear Regression by Calculating the Coefficients with Gradient Descent

The loss function for linear regression is
$$J(m, c) = \frac{1}{2n} \sum_{i=1}^n (y_i - (m x_i + c))^2$$
Now computing the derivatives of $J(m, c)$ with respect to the gradient and intercept, we have:
$$\frac{\partial}{\partial m}J(m, c)=-\frac{1}{n}\sum_{i=1}^n x_i(y_i - (m x_i + c))$$
$$\frac{\partial}{\partial c}J(m, c)=-\frac{1}{n}\sum_{i=1}^n (y_i - (m x_i + c))$$

We can implement the gradient descent method as follows:
```{r}
# TO DO
set.seed(1)
m= runif(2)
b= runif(1)
iterations = 1000
learning_rate = 0.01
n=nrow(X)
  
for(i in 1:iterations){
  
  prediction = X %*% m + b 
  error = as.numeric(Y - prediction)

  D_m = -1/n * apply(X * error,2,sum)  
  D_b = -1/n * sum(error)
  
  m = m - learning_rate * D_m
  b = b - learning_rate * D_b
}

prediction = b + as.matrix(X_test) %*% m
```

Using the coefficients from the gradient descent method, we make predictions on the testing dataset:
```{r}
# TO DO

```

We can also add the predictions obtained from the gradient descent method to the previous plot obtained:
```{r}
# TO DO

```